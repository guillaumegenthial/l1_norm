{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build the data\n",
    "\n",
    "See `build_data.py`\n",
    "\n",
    "We need to generate some data. Requirements:\n",
    "- enough to train, not too much for speed issues\n",
    "- train and dev sets ideally have the same distribution\n",
    "- test set distribution ideally a bit different\n",
    "- should be reproducible (fix random seed)\n",
    "- randomize dimension of vectors and scale of the distribution\n",
    "- saves data to files for future re-use of the model\n",
    "\n",
    "Then, the file `model/input_fn.py` takes care of the input data pipeline to the Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from build_data import get_one_example\n",
    "from build_data import export_to_file\n",
    "from build_data import save_dict_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(2018)\n",
    "\n",
    "# Define hyper parameters of the dataset\n",
    "DATA_PARENT_DIR = \"data\"\n",
    "# Sizes\n",
    "TRAIN_SIZE = 8000\n",
    "TEST_SIZE = 1000\n",
    "DEV_SIZE = 1000\n",
    "# Dimensions - test distribution is wider\n",
    "TRAIN_DIMENSION_LOW = 10\n",
    "TRAIN_DIMENSION_UPP = 200\n",
    "TEST_DIMENSION_LOW = 1\n",
    "TEST_DIMENSION_UPP = 400\n",
    "# Scales - test distribution is wider\n",
    "TRAIN_SCALE_LOW = 1\n",
    "TRAIN_SCALE_UPP = 10\n",
    "TEST_SCALE_LOW = 0.1\n",
    "TEST_SCALE_UPP = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the datasets\n",
    "data_train = [get_one_example(TRAIN_DIMENSION_LOW, TRAIN_DIMENSION_UPP,\n",
    "                              TRAIN_SCALE_LOW, TRAIN_SCALE_UPP)\n",
    "              for _ in range(TRAIN_SIZE)]\n",
    "data_dev = [get_one_example(TRAIN_DIMENSION_LOW, TRAIN_DIMENSION_UPP,\n",
    "                              TRAIN_SCALE_LOW, TRAIN_SCALE_UPP)\n",
    "              for _ in range(DEV_SIZE)]\n",
    "data_test = [get_one_example(TEST_DIMENSION_LOW, TEST_DIMENSION_UPP,\n",
    "                              TEST_SCALE_LOW, TEST_SCALE_UPP)\n",
    "              for _ in range(TEST_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to files\n",
    "if not os.path.exists(DATA_PARENT_DIR):\n",
    "    os.makedirs(DATA_PARENT_DIR)\n",
    "\n",
    "export_to_file(data_train, \"train.x\", \"train.y\")\n",
    "export_to_file(data_dev, \"dev.x\", \"dev.y\")\n",
    "export_to_file(data_test, \"test.x\", \"test.y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets properties in json file\n",
    "sizes = {\n",
    "    'train_size': len(data_train),\n",
    "    'dev_size': len(data_dev),\n",
    "    'test_size': len(data_test),\n",
    "}\n",
    "\n",
    "save_dict_to_json(sizes, os.path.join(DATA_PARENT_DIR, 'dataset_params.json'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the model\n",
    "\n",
    "See `model/model_fn.py`\n",
    "\n",
    "Obviously, the L1 norm of x_1, ...., x_n can be computed with the following graph\n",
    "\n",
    "```\n",
    "# shape = [batch_size, max_dimension in batch] (padded with zeros)\n",
    "out = input_vectors\n",
    "\n",
    "# Compute the absolute norm of every entry\n",
    "out = tf.nn.relu(out) + tf.nn.relu(-out)\n",
    "\n",
    "# Sum over the last dimension\n",
    "out = tf.reduce_sum(out, axis=-1)\n",
    "```\n",
    "\n",
    "It respects the requirement (only ReLUs, +, - and sum), but there is no learnable component...\n",
    "\n",
    "We can also test if we can learn the right connection for the 2 dense layers \n",
    "\n",
    "(learn this: [1, -1] -> relu -> [1, 1] -> reduce_sum or [a, -b] -> relu -> [1/a, 1/b] -> reduce_sum with a, b > 0).\n",
    "\n",
    "(or more generally \n",
    "\n",
    "$$ \\sum_i b_i relu( a_i x) $$ with $$ sum(a_i [a_i >0] * b_i[a_i > 0]) = 1 $$ and $$ sum(a_i[a_i < 0] * b_i[a_i < 0]) = -1 $$\n",
    "\n",
    "The operations of such a graph are summed up below: we need to take care of padding because of different dimensions in a batch\n",
    "\n",
    "\n",
    "```\n",
    "# shape = [batch_size, max_dimension in batch, 1]\n",
    "out = tf.expand_dims(out, axis=-1)\n",
    "\n",
    "out = tf.layers.dense(out, params.hidden_units, activation=tf.nn.relu, use_bias=False)\n",
    "out = tf.layers.dense(out, 1, activation=None, use_bias=False)\n",
    "\n",
    "# length is a tensor of shape [batch_size] of int where length[i] is the dim of the i-th example\n",
    "# shape = [batch_size, max dimension in batch, 1]\n",
    "mask = tf.expand_dims(tf.cast(tf.sequence_mask(lengths), tf.float32), axis=-1)\n",
    "# shape = [batch_size]\n",
    "predictions = tf.reduce_sum(tf.reduce_sum(outputs * mask, axis=-1), axis=-1)\n",
    "\n",
    "# Loss\n",
    "sum_of_dims = tf.reduce_sum(lengths)\n",
    "l2_loss = tf.reduce_mean(tf.square(predictions - labels))\n",
    "loss = l2_loss / tf.cast(sum_of_dims, tf.float32) # loss per component for indep to other hp\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "We can make one interesting observation about the problem. If all units have the same sign, then the negative entries of the vectors will never matter in the loss function. In that case, if we assign a_i that verifies\n",
    "\n",
    "$$ \\sum(a_i [a_i >0] * b_i[a_i > 0]) = 2 $$\n",
    "\n",
    "then, we get a local minimum. The way we sample data indeed creates roughly the same amount of negative components and positive components, meaning that $ - \\sum_{x_i < 0} x_i \\approx \\sum_{x_i > 0} x_i $, and thus, a way to approximate the L1 norm is just to compute $ 2 *  \\sum_{x_i > 0} x_i $ (the same analysis is also valid for the negative entries, by symetry)\n",
    "\n",
    "Thus, if we use a higher number of hidden units (say 20), we can break the symmetry, and all the components (positive and negative) do matter to the cost function and we are less likely to get stuck in a local optimum where we only take positive (or negative) components into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import Params\n",
    "from model.utils import set_logger\n",
    "from model.training import train_and_evaluate\n",
    "from model.input_fn import input_fn\n",
    "from model.input_fn import load_dataset_from_text\n",
    "from model.model_fn import model_fn\n",
    "\n",
    "# Set the random seed for the whole graph for reproductible experiments\n",
    "tf.reset_default_graph()\n",
    "# Removing random seed that gave favorable initialization\n",
    "# tf.set_random_seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import hyperparams and setup the logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 10, 'model_version': 'trainable', 'hidden_size': 20, 'save_summary_steps': 100, 'train_size': 8000, 'dev_size': 1000, 'test_size': 1000}\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"experiments/base_model\"\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Load the parameters from the experiment params.json file in model_dir\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = Params(json_path)\n",
    "\n",
    " # Load the parameters from the dataset, that gives the size etc. into params\n",
    "json_path = os.path.join(data_dir, 'dataset_params.json')\n",
    "params.update(json_path)\n",
    "\n",
    "# Set the logger\n",
    "set_logger(os.path.join(model_dir, 'train.log'))\n",
    "print(params.dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the vectors and l1 norm with tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating the datasets...\n"
     ]
    }
   ],
   "source": [
    "# Get paths for vocabularies and dataset\n",
    "path_train_x = os.path.join(data_dir, 'train.x')\n",
    "path_train_y = os.path.join(data_dir, 'train.y')\n",
    "path_eval_x = os.path.join(data_dir, 'dev.x')\n",
    "path_eval_y = os.path.join(data_dir, 'dev.y')\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Creating the datasets...\")\n",
    "train_x = load_dataset_from_text(path_train_x)\n",
    "train_labels = load_dataset_from_text(path_train_y)\n",
    "eval_x = load_dataset_from_text(path_eval_x)\n",
    "eval_labels = load_dataset_from_text(path_eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create the input function and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- done.\n",
      "Creating the model...\n",
      "- done.\n"
     ]
    }
   ],
   "source": [
    "# Specify other parameters for the dataset and the model\n",
    "params.eval_size = params.dev_size\n",
    "params.buffer_size = params.train_size # buffer size for shuffling\n",
    "\n",
    "# Create the two iterators over the two datasets\n",
    "train_inputs = input_fn('train', train_x, train_labels, params)\n",
    "eval_inputs = input_fn('eval', eval_x, eval_labels, params)\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "# Define the models (2 different set of nodes that share weights for train and eval)\n",
    "logging.info(\"Creating the model...\")\n",
    "train_model_spec = model_fn('train', train_inputs, params)\n",
    "eval_model_spec = model_fn('eval', eval_inputs, params, reuse=True)\n",
    "logging.info(\"- done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the model\n",
    "\n",
    "\n",
    "### Training and Hyperparameters\n",
    "\n",
    "- Applying Dropout would not make sense as the architecture is as minimalist as possible. L2 regularization would introduce a bias, even though it would penalize high values of the weights (that work, [a, -b] and [1/a, 1/b] are valid weights for all a for our 2 layer network)\n",
    "- Relevant hyperparameters are batch_size, learning_rate, optimization method.\n",
    "- Training loss is the L2 loss between the average per component of the predicted L1 norm and the gold L1 norm\n",
    "- We take the average per component and per batch so that changing other hyperparameters does not impact the choice of learning_rate and batch_size\n",
    "\n",
    "\n",
    "The accuracy here is just the negative L2 loss (thus the higher the better, used to select the best weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epoch(s)\n",
      "Epoch 1/10\n",
      "100%|██████████| 250/250 [00:05<00:00, 43.81it/s, loss=0.885]  \n",
      "- Train metrics: loss: 91.063 ; neg_l2_loss: -12655.478 ; accuracy: -91.063\n",
      "- Eval metrics: loss: 0.399 ; neg_l2_loss: -55.555 ; accuracy: -0.399\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-1\n",
      "Epoch 2/10\n",
      "100%|██████████| 250/250 [00:05<00:00, 44.57it/s, loss=0.049]\n",
      "- Train metrics: loss: 0.274 ; neg_l2_loss: -32.423 ; accuracy: -0.274\n",
      "- Eval metrics: loss: 0.123 ; neg_l2_loss: -18.861 ; accuracy: -0.123\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-2\n",
      "Epoch 3/10\n",
      "100%|██████████| 250/250 [00:05<00:00, 44.29it/s, loss=0.013]\n",
      "- Train metrics: loss: 0.070 ; neg_l2_loss: -7.979 ; accuracy: -0.070\n",
      "- Eval metrics: loss: 0.033 ; neg_l2_loss: -4.781 ; accuracy: -0.033\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-3\n",
      "Epoch 4/10\n",
      "100%|██████████| 250/250 [00:05<00:00, 43.61it/s, loss=0.001]\n",
      "- Train metrics: loss: 0.009 ; neg_l2_loss: -0.970 ; accuracy: -0.009\n",
      "- Eval metrics: loss: 0.001 ; neg_l2_loss: -0.205 ; accuracy: -0.001\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-4\n",
      "Epoch 5/10\n",
      "100%|██████████| 250/250 [00:05<00:00, 41.77it/s, loss=0.000]\n",
      "- Train metrics: loss: 0.001 ; neg_l2_loss: -0.091 ; accuracy: -0.001\n",
      "- Eval metrics: loss: 0.000 ; neg_l2_loss: -0.009 ; accuracy: -0.000\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-5\n",
      "Epoch 6/10\n",
      "100%|██████████| 250/250 [00:06<00:00, 41.42it/s, loss=0.000]\n",
      "- Train metrics: loss: 0.000 ; neg_l2_loss: -0.005 ; accuracy: -0.000\n",
      "- Eval metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-6\n",
      "Epoch 7/10\n",
      "100%|██████████| 250/250 [00:06<00:00, 40.89it/s, loss=0.000]\n",
      "- Train metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n",
      "- Eval metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-7\n",
      "Epoch 8/10\n",
      "100%|██████████| 250/250 [00:06<00:00, 40.82it/s, loss=0.000]\n",
      "- Train metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n",
      "- Eval metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-8\n",
      "Epoch 9/10\n",
      "100%|██████████| 250/250 [00:06<00:00, 39.66it/s, loss=0.000]\n",
      "- Train metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n",
      "- Eval metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n",
      "- Found new best accuracy, saving in experiments/base_model/best_weights/after-epoch-9\n",
      "Epoch 10/10\n",
      "100%|██████████| 250/250 [00:06<00:00, 38.96it/s, loss=0.000]\n",
      "- Train metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n",
      "- Eval metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(train_model_spec, eval_model_spec, model_dir, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the runs reach a zero error during training. It can happen that after reaching a good minimum, the loss goes up a little: it can be explained by a noisy batch with stronger disymetry than the weights that cause the weights to change a bit in a non-favorable way (that kind of thing could be solved by having a learning rate decay, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating the dataset...\n"
     ]
    }
   ],
   "source": [
    "# Reset the default graph\n",
    "from model.evaluation import evaluate\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Get paths for vocabularies and dataset\n",
    "path_eval_x = os.path.join(data_dir, 'dev.x')\n",
    "path_eval_y = os.path.join(data_dir, 'dev.y')\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Creating the dataset...\")\n",
    "test_x = load_dataset_from_text(path_eval_x)\n",
    "test_labels = load_dataset_from_text(path_eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- done.\n",
      "Creating the model...\n",
      "- done.\n"
     ]
    }
   ],
   "source": [
    "# Specify other parameters for the dataset and the model\n",
    "params.eval_size = params.test_size\n",
    "\n",
    "# Create iterator over the test set\n",
    "inputs = input_fn('eval', test_x, test_labels, params)\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "# Define the model\n",
    "logging.info(\"Creating the model...\")\n",
    "model_spec = model_fn('eval', inputs, params, reuse=False)\n",
    "logging.info(\"- done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from experiments/base_model/best_weights/after-epoch-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from experiments/base_model/best_weights/after-epoch-9\n",
      "- Eval metrics: loss: 0.000 ; neg_l2_loss: -0.000 ; accuracy: -0.000\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting evaluation\")\n",
    "evaluate(model_spec, model_dir, params, \"best_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Observations\n",
    "\n",
    "- Both models achieve 0 L2 loss between the two L1 norms (neural and gold).\n",
    "- Some sensibility to the learning_rate, resolved with hyperparameter search and the use of Adam.\n",
    "- Ability to generalize as the distribution of the test set is slightly different\n",
    "\n",
    "We can look at the actual predicted values and check that we get what we expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from experiments/base_model/best_weights/after-epoch-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from experiments/base_model/best_weights/after-epoch-9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "     # Reload weights from the weights subdirectory\n",
    "    save_path = os.path.join(model_dir, \"best_weights\")\n",
    "    if os.path.isdir(save_path):\n",
    "        save_path = tf.train.latest_checkpoint(save_path)\n",
    "    saver.restore(sess, save_path)\n",
    "\n",
    "    a, b = sess.run(variables)\n",
    "a = np.squeeze(a)\n",
    "b = np.squeeze(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the values found verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000004788143997"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a[a>0] * b[a>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999996540136635"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a[a<0] * b[a<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
